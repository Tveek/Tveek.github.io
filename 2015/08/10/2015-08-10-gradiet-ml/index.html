<!DOCTYPE html>
<html lang="">
  <head>
    
<!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="description" content="梯度下降算法详解"/>







  <link rel="alternate" href="/default" title="Note">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=2.3.x" />



<link rel="canonical" href="http://yoursite.com/2015/08/10/2015-08-10-gradiet-ml/"/>


<meta name="description" content="梯度下降在机器学习领域是用得比较多的算法.简单的说就是用一大堆数据去解一个未知的方程,亦或是用一大堆数据取拟合一个方程. $ $">
<meta name="keywords">
<meta property="og:type" content="article">
<meta property="og:title" content="梯度下降算法详解">
<meta property="og:url" content="http://yoursite.com/2015/08/10/2015-08-10-gradiet-ml/index.html">
<meta property="og:site_name" content="Note">
<meta property="og:description" content="梯度下降在机器学习领域是用得比较多的算法.简单的说就是用一大堆数据去解一个未知的方程,亦或是用一大堆数据取拟合一个方程. $ $">
<meta property="og:updated_time" content="2017-04-17T04:27:13.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="梯度下降算法详解">
<meta name="twitter:description" content="梯度下降在机器学习领域是用得比较多的算法.简单的说就是用一大堆数据去解一个未知的方程,亦或是用一大堆数据取拟合一个方程. $ $">


<link rel="stylesheet" type="text/css" href="/css/style.css?v=2.3.x" />







<script>
  var CONFIG = {
    search: false,
    searchPath: "/search.xml",
    fancybox: false,
    toc: false,
  }
</script>




  
  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?8466adbc25665cf7ee1a15e4fcb73643";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>


  <script type="text/javascript">
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-74273646-1', 'auto');
        ga('send', 'pageview');
  </script>



    <title> 梯度下降算法详解 · Note </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  </head>

  <body><div id="mobile-navbar" class="mobile-navbar">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="mobile-header-logo">
    <a href="/." class="logo">Note</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>

<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    
      <a href="/">
        <li class="mobile-menu-item">
          
          
            Home
          
        </li>
      </a>
    
      <a href="/archives/">
        <li class="mobile-menu-item">
          
          
            Archives
          
        </li>
      </a>
    
      <a href="/categories">
        <li class="mobile-menu-item">
          
          
            Categories
          
        </li>
      </a>
    
  </ul>
</nav>

    <div class="container" id="mobile-panel">
      <header id="header" class="header"><div class="logo-wrapper">
  <a href="/." class="logo">Note</a>
</div>

<nav class="site-navbar">
  
    <ul id="menu" class="menu">
      
        <li class="menu-item">
          <a class="menu-item-link" href="/">
            
            
              Home
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/archives/">
            
            
              Archives
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/categories">
            
            
              Categories
            
          </a>
        </li>
      
      
    </ul>
  
</nav>

      </header>

      <main id="main" class="main">
        <div class="content-wrapper">
          <div id="content" class="content">
            
  
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          梯度下降算法详解
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          Aug 10, 2015
        </span>
      </div>
    </header>

    
    

    <div class="post-content">
      
        <p>梯度下降在机器学习领域是用得比较多的算法.简单的说就是用一大堆数据去解一个未知的方程,亦或是用一大堆数据取拟合一个方程. $ $ <a id="more"></a></p>
<h1 id="基本概念">基本概念</h1>
<p>首先我们知道一堆数据,但是不知道他们的具体关系(为了方便起见,假设是<strong>线性关系</strong>). <span class="math">\[
x_{1}^{(1)},x_{2}^{(1)},\cdots,x_{n}^{(1)},y^{(1)}\\
\vdots\\
x_{1}^{(m)},x_{2}^{(m)},\cdots,x_{n}^{(m)},y^{(m)}
\]</span> 由于我们假设这些数据是线性关系,因此不妨设其方程为:</p>
<p><span class="math">\[
h_{\theta}(x)=\theta_{0}*1+\theta_{1}x_{1}+\theta_{2}x_{2}+\dots+\theta_{n}x_{n}
\]</span></p>
<p>这里的<span class="math">\(h\)</span>其实代指的是<span class="math">\(y\)</span>,只不过<span class="math">\(y\)</span>在后面用到了,为了不重复,就用<span class="math">\(h\)</span>了.我们把其写成向量的形式:</p>
<p><span class="math">\[
h(x)=\sum_{i=0}^{n}\theta_{i}x_{i}=\theta ^{T}x
\]</span></p>
<p>接下来讲一下,如何求得<span class="math">\(\theta\)</span>,其实就是要这个方程能尽可能的表示这些数据.那么只要使得<span class="math">\(y\)</span>和<span class="math">\(h\)</span>之间的差距尽可能的小就好了,于是构建下面的<strong>损失函数(Cost Function)</strong></p>
<p><span class="math">\[
J(\theta)=\frac{1}{2}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^{2}
\]</span></p>
<p>很显然<span class="math">\(J(\theta)\)</span>是凸函数,有极小值.我们只要想办法使这个极小值趋向于0就好了.接下来就是<strong>梯度下降(gradient descent)</strong>的核心步骤了</p>
<p><span class="math">\[
\theta_{j}:=\theta_{j}-\alpha\frac{\partial }{\partial \theta_{j}}J(\theta)
\]</span></p>
<p>稍微解释一下,上式的意思就是使得<span class="math">\(\theta_{j}\)</span>几乎不在变化(就是小于某个阀值).其中<span class="math">\(\alpha\)</span>是学习率,是固定的.但是这个公式的严格证明小编不太会了.最后写出上述的最简表达式</p>
<p>具体的证明可以看一下Andrew Ng机器学习的课程</p>
<p><span class="math">\[
\theta_{j}:=\theta_{j}+\alpha(y^{(i)}-h_{\theta}(x^{(i)}))x_{j}^{(i)}
\]</span></p>
<h1 id="批量梯度下降">批量梯度下降</h1>
<p>批量梯度下降(bath gradient descent)顾名思义就是每一次运算都将所有的数据投入运算中.这里给出算法的公式</p>
<p>Repeat until convergence{</p>
<p><span class="math">\[
\theta_{j}:=\theta_{j}+\alpha\sum_{i=1}^{m}(y^{(i)}-h_{\theta}(x^{(i)}))x_{j}^{(i)} \quad(for\quad every\quad j)
\]</span></p>
<p>}<br> 这些理论的公式,最终还是要写出算法,代码才是王道. <figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">Gradient_Descent_Batch</span><span class="params">(X,y)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    batch gradient descent</div><div class="line">    :type X:    numpy.array</div><div class="line">    :param X:   dataset(m*n)</div><div class="line">    :type y:    numpy.array</div><div class="line">    :param y:   labels(m*1)</div><div class="line">    """</div><div class="line">    <span class="comment">#  insert one column with 1(这里插入一列是因为x0也是数据的一部分)</span></div><div class="line">    X=np.insert(X,<span class="number">0</span>,<span class="number">1</span>,axis=<span class="number">1</span>)</div><div class="line">    m=X.shape[<span class="number">0</span>]</div><div class="line">    n=X.shape[<span class="number">1</span>]</div><div class="line"></div><div class="line">    <span class="comment"># learning rate(这个选择很重要,要不然无法收敛)</span></div><div class="line">    alpha=<span class="number">0.0001</span></div><div class="line">    max_interation=<span class="number">100</span></div><div class="line"></div><div class="line">    <span class="comment"># init theta</span></div><div class="line">    theta=np.ones((n,<span class="number">1</span>))</div><div class="line"></div><div class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> range(<span class="number">0</span>,max_interation):</div><div class="line">        grad=X.T.dot((X.dot(theta)-y))</div><div class="line">        old_theta=theta</div><div class="line">        theta=theta-alpha*grad</div><div class="line">        diff_theta=theta-old_theta</div><div class="line">        <span class="keyword">if</span>(np.sqrt(diff_theta.T.dot(diff_theta))&lt;<span class="number">0.00000001</span>):</div><div class="line">            <span class="keyword">break</span></div><div class="line">    <span class="keyword">return</span> theta</div></pre></td></tr></table></figure></p>
<h1 id="随机梯度下降">随机梯度下降</h1>
<p>随机梯度下降(stochastic gradient descent)与批量梯度下降最大的区别就是每次计算只取一组数据.我们还是先给公式<br> Loop{<br>   for i =1 to m {<br></p>
<p><span class="math">\[
\theta_{j}:=\theta_{j}+\alpha(y^{(i)}-h_{\theta}(x^{(i)}))x_{j}^{(i)}
\]</span></p>
<p>  }<br> }<br> 既然公式完了,那么我们就上代码 <figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">Gradient_Descent_Rand</span><span class="params">(X,y)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    stochastic gradient descent</div><div class="line">    :type X:    numpy.array</div><div class="line">    :param X:   a dataset(m*n)</div><div class="line">    :type y:    numpy.array</div><div class="line">    :param y:   a labels(m*1)</div><div class="line">    """</div><div class="line">    X=np.insert(X,<span class="number">0</span>,<span class="number">1</span>,axis=<span class="number">1</span>)</div><div class="line">    m=X.shape[<span class="number">0</span>]</div><div class="line">    n=X.shape[<span class="number">1</span>]</div><div class="line">    theta=np.zeros((n,<span class="number">1</span>))</div><div class="line">    <span class="comment"># ndarray 转 matrix</span></div><div class="line">    X=np.matrix(X)</div><div class="line">    y=np.matrix(y)</div><div class="line">    theta=np.matrix(theta)</div><div class="line"></div><div class="line">    <span class="comment"># 学习率</span></div><div class="line">    alpha=<span class="number">0.000001</span></div><div class="line"></div><div class="line">    max_interation=<span class="number">100000</span></div><div class="line"></div><div class="line"></div><div class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> range(<span class="number">0</span>,max_interation):</div><div class="line">        old_theta=theta</div><div class="line">        <span class="comment"># 随机指定一个样本</span></div><div class="line">        j=randint(<span class="number">0</span>,m<span class="number">-1</span>)</div><div class="line">        <span class="comment"># 这里就体现了批量和随机的区别</span></div><div class="line">        grad=X[j,:].T*(X[j,:]*theta-y[j,:])</div><div class="line">        theta=theta-alpha*grad</div><div class="line">        diff_theta=theta-old_theta</div><div class="line">        <span class="keyword">if</span>(np.sqrt(diff_theta.T*diff_theta)&lt;<span class="number">0.00000001</span>):</div><div class="line">            <span class="keyword">break</span></div><div class="line">    <span class="keyword">return</span> theta</div></pre></td></tr></table></figure></p>

      
    </div>

    
      
      



      
      
    

    
      <footer class="post-footer">
        
        
        
  <nav class="post-nav">
    
    
      <a class="next" href="/2015/08/12/2015-08-12-normal-newton-ml/">
        <span class="next-text nav-default">机器学习其他最小化方法</span>
        <span class="prev-text nav-mobile">Next</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>

      </footer>
    

  </article>


          </div>
          
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="2015/08/10/2015-08-10-gradiet-ml/"
           data-title="梯度下降算法详解" data-url="http://yoursite.com/2015/08/10/2015-08-10-gradiet-ml/">
      </div>
    
  </div>


        </div>  
      </main>

      <footer id="footer" class="footer">

  <div class="social-links">
    
      
        
          <a href="mailto:algoreek@gmail.com" class="iconfont icon-email" title="email"></a>
        
      
    
      
    
      
    
      
    
      
    
      
    
      
        
          <a href="https://github.com/tveek" class="iconfont icon-github" title="github"></a>
        
      
    
      
    
      
        
          <a href="https://www.zhihu.com/people/tveek/activities" class="iconfont icon-zhihu" title="zhihu"></a>
        
      
    
      
    
    
    
      
      <a href="/atom.xml" class="iconfont icon-rss" title="rss"></a>
    
  </div>


<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://hexo.io/">Hexo</a>
  </span>
  
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/ahonn/hexo-theme-even">Even</a>
  </span>

  <span class="copyright-year">
    
    &copy; 
     
      2015 - 
    
    2017

    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">Tveek</span>
  </span>
</div>
      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>

    
  
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"tanpan123"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>


  
  




    




  
    <script type="text/javascript" src="/lib/jquery/jquery-3.1.1.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/slideout/slideout.js"></script>
  

  


    <script type="text/javascript" src="/js/src/even.js?v=2.3.x"></script>
<script type="text/javascript" src="/js/src/bootstrap.js?v=2.3.x"></script>

    
    1<!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


  </body>
</html>
